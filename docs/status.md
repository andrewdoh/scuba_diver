---
layout: default
title: Status
---

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/otJyhsSgTw8" frameborder="0" allowfullscreen>
</iframe>
</p>
### Technical Description ###

## Project Summary ##
When we first began formulating an idea for the project, we initially wanted to teach the agent to learn how to shoot a air and bow efficiently using a convolutional neural network. We dove in head first trying to learn all we could about how to use a CNN with minecraft. However, as time progressed, we began to see we were way out of our depths in terms of knowledge about an already very complex model. So, we had to reevaluate our goals and decided it was best to reduce the complexity of our project but still have an interesting idea to apply to minecraft. With the class being about reinforcement learning, we thought it would a be cool idea to use Q-learning as the basis of our project. The general idea is to use a single Q-learning algorithm and let the agent learn and gather data. Then, we will implement double Q-learning algorithm to fix the inefficiencies with just a single Q-learning algorithm. Once this is done we will make a comparison of the two types, highlighting the pros, cons and differences between them. A more general explanation of our new project from the minecraft perspective is an agent that explores a 3D underwater maze in search treasure but with a limited amount of resources. This might sounds like assignment 2 but there are some key differences to which we apply the algorithm and variables that provide for some very interesting problems. The first problem being that to navigate a 3D maze, we must allow the agent to move in 3 dimensions. Instead of having the 4 standard states (forward, backward, left, right), we now have 6 states which includes moving up and moving down. The second major difference is that we introduced a new resource that needs to be managed by the agent in the form of air. Now, the agent has to decided whether it has enough air to keep exploring and find more treasure or to reach the goal without drowning. Currently, our group is testing the agent with only two floors enabled because it is more efficient for testing our algorithm and it has a state space large enough for us to understand its scale but small enough so that it won’t take us a long time to get the agent to final goal. 

## Approach ##
Before discussing the main algorithm we will be using to train our agent, let’s look more in depth at our MDP:

<img src="images/main environment.png" width="400px">

Our agent is exploring a 3-Dimensional, underwater maze, and as such, each of our states will include the current x,y,z location in the Minecraft map. In addition, because our agent needs to keep track of how much air it has left, we will associate the amount of breath it has with the following keywords: “high”, “medium”, “low”. So an example of a state may be x="1142.5" y="25" z="-481.5", breath=”high”. With this, you can see that even in a small environment, say a 5x5x5 cube of water, the size of our state space quickly heightens to 375 different states (5*5*5*3). However, having wall-like structures to create mazes decreases the locations in which the agent can travel to within this cube, and helps to reduce the overall state space. 
Since this is a 3D maze, we are allowing the agent to maneuver in a 3D manner. Thus, its actions consist of the following: right, left, forward, backwards, up, down. Unfortunately, because of the way Minecraft works, there is no simple command to move up and down freely in water via Malmo commands. Furthermore, if the agent is not always standing on some block, it may begin to sink, which could be potentially harmful to how it updates/learns. We solved this issue by creating multiple floors, so the agent is always ‘grounded’, and move up and down between levels by using the ‘teleport’ command. 

It should also be noted that in order to make for a more intriguing learner, we decided not to make the mazes on each floor consistent (i.e. at a position (x,y) on the first floor, a wall may be present, but at the same position (x,y) on the second floor, a wall may be non-existent). This limits the actions our agent can take given a particular state, so we constructed the following ‘observation grid’ for our agent in order to make sure that we can teleport up/down a level.

<img src="images/floor and air bubbles.png" width="400px">

As our agent explores the maze, it can come across three different types of rewards: receiving an ender pearl (+10 points), finding an air pocket (+10 if the agent is ‘low’ on air, +5 is the agent has ‘medium’ air, and -1 if the agent has ‘a high’ amount of air), as well as finding its goal state, the redstone_block (+100). We assign the agent different rewards for finding an air pocket based off how much air it has left in order to encourage it not only visit an air pocket when it truly needs it. In addition, in order to encourage the agent to find the goal state more quickly (which is located on the bottom-most floor), we decided to give it -1 reward for every step it takes, and +1 reward when it moves down a floor level.

As one of the first breakthroughs in reinforcement learning, the Q-learning algorithm has very unique and interesting properties (Watkins, 1989), which made it an easy choice for the basis of our project. With this in mind, we decided to do some research on different approaches to the Q-learning algorithm. We soon came across the method of ‘Double Q-Learning’ and found that it would fit nicely with the idea of our project. Double Q-learning is just what it sounds like, it’s using two Q-tables instead of just one. However, the advantages are rather intriguing; by separating out reward values into two separate Q-tables, we can inherently separate the value of a state from the value of an action taken to get there. Thus, one Q-table will essentially represent that value of a given state (independent of action), and the other will represent the value of an action given a particular state. This is beneficial for the framework of our project because choosing a particular action may not necessarily be as important as the value of the state itself. For instance, the amount of air our agent has is constantly depleting, and depending on how much air it has left (which is included in the description of its current state), it may choose to travel to a nearby air space, or to simply ignore it and continue exploring other areas in more depth. And though we may find that taking a step forward leads us to an air pocket, if we happen to have a large amount of air left, going to the space may essentially be a waste of time. Thus, its current state may be more beneficial in determining what to do next versus where a particular action takes it.   

Though Double Q-learning does not guarantee for the agent to converge any more quickly, it does allow it process complex state spaces more efficiently. One of the key differences between this method and using a single Q-table is that when choosing an action, instead of choosing the maximum action with probability (1-epsilon), we average over the values of both of the Q-tables, and take the maximum value from the results. By separating out values and averaging among them, this process helps to correct the single Q-learning algorithm’s tendencies to overestimate the optimal action choice. Another difference is that when updating the Q-tables, we randomly choose to update only one on each round (each have a 50% probability). Overall, we believe this method will allow our agent to better analyze the environment in which it’s placed.

 
## Evaluation ##
The evaluation of our project will be an analysis of a single Q-learning algorithm and double Q-learning algorithm followed by a comparison between the two. We will begin by collecting various types of data for each algorithm. Since we're using a reinforcement learning algorithm, the main metrics by which we will gauge our project will in terms of $$\alpha$$, actions, episodes, and rewards. During each episode we take note of the every aspect regarding our agent. Using this data, we will analyze and perform a variety of different metrics both quantitative and qualitative to demonstrate that they are performing as intended. Following this, we will compare the two algorithms showing the deficiencies with a single Q-learning algorithm and the improvements made with the double Q-learning algorithm. The single Q-learning algorithm tends to overestimating current action values given that is a greedy algorithm which tends to lead to maximization bias. This would be an interesting event to see and analyze the circumstances leading to as the number of episodes increase. Additionally, when our prototype is completed, an analysis will be done to see how quantitatively how the problems with a 3D state space were effect when function approximation is applied. In closing, the Q-learning algorithm has an abundance of interesting properties which is why we chose to use this algorithm. 


## Remaining Goals and Challenges ##
While it’s a bit premature to state, the remaining goals and challenges we have for remaining days till the final submission are to make the necessary modifications to make our prototype fully functional. More specifically, as our project currently stands, it currently never converges to the optimal policy from the small amounts of testing we’ve done. With the modifications we’ve made to make this project to make it interesting, namely, the change from 2D action space to 3D, this was to be expected. To fix this problem, we initially had thought of doing some form of function approximation methods such as a neural network, linear approximation and etc. However, after some more research and meetings with the teaching assistant we’ve come to realize that our initially ideas were a bit overkill for the problem. While the ideas of using a neural network or some other popular form of function approximation seem very appealing and exciting, the sheer amount of time needed to understand and produce a functioning prototype are beyond the amount of time we have for this project. So instead, to fix this problem, we create a more simple function approximator that computes the target in terms of distances to items. From our analysis, we can see that the agent, with it’s current possible action space, seems to take unnecessary actions that yield no additional information. For example, given an episode in the initial rounds, the agent will take a path to previously visited location with no treasures or pockets of air and just randomly walk and get stuck in that particular location for an extended period of time instead of going to exploring for more treasure. This would seem to indicate that the amount of data currently being gathered is too much for the agent to handle, which is where our function approximation comes in. Rather than takes unnecessary actions our function approximator will target distances to items to alleviate the problem our agent has with taking unnecessary explorations of previously visited states. This naturally brings into question how the function will affect the amount of exploration the agent will do now. We hypothesis that there will a slight decrease in amount of exploration that will be done but not so much as to affect negatively the cumulative reward. To verify our expectations, we will provide a comparison of the different implementations we’ve made with a variety of different graphs. In regards to our current experiences, we anticipate the problems that will arise by the time the final report is due are that thinking of an idea and implementing it encompass many more problems that one might expect. While idea are simple to state, there are many problem that arise when the combination of so many different technologies are involved. We’ve realized that now after finally becoming somewhat familiar with the Malmo API. While we don’t know for certain how crippling this problem might be, we believe we’ve taken the necessary precautions to maximize our chances of completing our goal, namely, not implementing a already complex method(Neural Network), to solve a relatively simple problem. We do expect problems to arise with the more relatively simple function approximator but expect that by reducing the complexity of the proposed method that the problems that arise should be simple to solve as well. In conclusion, with our current progression of the project we don’t expect the remaining problems will cause a roadblock to our remaining goals like we previously had with our initial assumptions and expect to finish the project in a timely manner. 

### Algorithm ###
#### Q-learning algorithm ####
$$ Q(s_t,a_t)  \leftarrow \underbrace{Q(s_t,a_t)}_{\text{old value}} + \underbrace{\alpha_t}_{\text{learning rate}} \cdot \Bigg(\overbrace{\underbrace{r_{t+1}}_{\text{reward}} + \underbrace{\gamma}_{\text{discount factor}} \cdot \underbrace{\max\limits_{a} Q(s_{t+1},a)}_{\text{estimate of optimal future value}}}^{\text{learned value}}   - \underbrace{Q(s_t,a_t)}_{\text{old value}}  \Bigg)$$
#### Psuedocode ####
<img src="images/single_q_learning.png" width="600px">

#### Double Q-learning algorithm ####
$$ Q_{1}(s_t,a_t)  \leftarrow \underbrace{Q_{1}(s_t,a_t)}_{\text{old value}} + \underbrace{\alpha_t}_{\text{learning rate}} \cdot \Bigg(\overbrace{\underbrace{r_{t+1}}_{\text{reward}} +  \underbrace{\gamma}_{\text{discount factor}} \cdot \underbrace{Q_{2} \big(s_{t+1},\max\limits_{a} Q_{1}(s_{t+1},a)\big)}}^{\text{learned value}}_{\text{estimate of optimal future value}}  - \underbrace{Q_{1}(s_t,a_t)}_{\text{old value}}  \Bigg)$$
$$ Q_{2}(s_t,a_t)  \leftarrow \underbrace{Q_{2}(s_t,a_t)}_{\text{old value}} + \underbrace{\alpha_t}_{\text{learning rate}} \cdot \Bigg(\overbrace{\underbrace{r_{t+1}}_{\text{reward}} +  \underbrace{\gamma}_{\text{discount factor}} \cdot \underbrace{Q_{1} \big(s_{t+1},\max\limits_{a} Q_{2}(s_{t+1},a)\big)}}^{\text{learned value}}_{\text{estimate of optimal future value}} - \underbrace{Q_{2}(s_t,a_t)}_{\text{old value}}  \Bigg)$$
#### Psuedocode ####
<img src="images/double_q_learning.png" width="600px">

<img src="images/mdp.png" width="200px">

<img src="images/alpha.png" width="600px">
<img src="images/asymptotic.png" width="600px">
<img src="images/singleq.png" width="600px">
<img src="images/singlevsdouble.png" width="600px">
