---
layout: default
title: Status
---

### Technical Description ###
## Project Summary ##
When we first began formulating an idea for the project, we initially wanted to teach the agent to learn how to shoot a air and bow efficiently using a convolutional neural network. We dove in head first trying to learn all we could about how to use a CNN with minecraft. However, as time progressed, we began to see we were way out of our depths in terms of knowledge about an already very complex model. So, we had to reevaluate our goals and decided it was best to reduce the complexity of our project but still have an interesting idea to apply to minecraft. With the class being about reinforcement learning, we thought it would a be cool idea to use Q-learning as the basis of our project. The general idea is to use a single Q-learning algorithm and let the agent learn and gather data. Then, we will implement double Q-learning algorithm to fix the inefficiencies with just a single Q-learning algorithm. Once this is done we will make a comparison of the two types, highlighting the pros, cons and differences between them. A more general explanation of our new project from the minecraft perspective is an agent that explores a 3D underwater maze in search treasure but with a limited amount of resources. This might sounds like assignment 2 but there are some key differences to which we apply the algorithm and variables that provide for some very interesting problems. The first problem being that to navigate a 3D maze, we must allow the agent to move in 3 dimensions. Instead of having the 4 standard states (forward, backward, left, right), we now have 6 states which includes moving up and moving down. The second major difference is that we introduced a new resource that needs to be managed by the agent in the form of air. Now, the agent has to decided whether it has enough air to keep exploring and find more treasure or to reach the goal without drowning. Currently, our group is testing the agent with only two floors enabled because it is more efficient for testing our algorithm and it has a state space large enough for us to understand its scale but small enough so that it won’t take us a long time to get the agent to final goal. 
## Approach ##
For our project, we used double q-learning to explore a custom made map which features a tower that is located under the water. The underwater tower has a total of 6 floors but currently, we are using only 2 floors to demonstrate our AI because it helps with how long it takes the AI to get to the goal and it lowers the numbers of states by a large amount. The maps features a few main points. The starting block is an emerald block located on the top floor of the tower. The end block is a redstone block that is currently on the second floor of the tower but will be moved to the last floor for our final project. If the agent reaches the redstone block, the agent will clear the maze and is rewarded 100. Throughout the maze, there are dispensers that will spit out an ender pearl if the agent steps on the pressure plate next to the dispenser. The ender pearl will reward 10 once picked up by the agent. Same as the dispenser, there is air in the form of wooden doors scattered throughout the floors for the agent to use to survive. However, the air does not give any reward because that would just make our agent hover around the spots with air and there will be no incentive for the agent to keep exploring for the final goal.
## Evaluation ##
The evaluation of our project will be an analysis of a single Q-learning algorithm and double Q-learning algorithm followed by a comparison between the two. We will begin by collecting various types of data for each algorithm. Since we're using a reinforcement learning algorithm, the main metrics by which we will gauge our project will in terms of $$\alpha$$, actions, episodes, and rewards. During each episode we take note of the every aspect regarding our agent. Using this data, we will analyze and perform a variety of different metrics both quantative and qualitative to demonstrate that they are performing as intended. Following this, we will compare the two algorithms showing the deficienes with a single Q-learning algorithm and the improvements made with the double Q-learning algorithm. The single Q-learning algorithm tends to overestimating current action values given that is a greedy algorithm which tends to lead to maximization bias. This would be an interesting event to see and analyze the circumstances leading to as the number of episodes increase. Additionally, when our prototype is completed, an analysis will be done to see how quantativly how the problems with a 3D state space were effect when function approximation is applied. In closing, the Q-learning algorithm has a wealth of interesting properties which is why we chose to use this algorithm.


## Remaining Goals and Challenges ##
While it’s a bit premature to state, the remaining goals and challenges we have for remaining days till the final submission are to make the necessary modifications to make our prototype fully functional. More specifically, as our project currently stands, it currently never converges to the optimal policy from the small amounts of testing we’ve done. With the modifications we’ve made to make this project to make it interesting, namely, the change from 2D action space to 3D, this was to be expected. To fix this problem, we initially had thought of doing some form of function approximation methods such as a neural network, linear approximation and etc. However, after some more research and meetings with the teaching assistant we’ve come to realize that our initially ideas were a bit overkill for the problem. While the ideas of using a neural network or some other popular form of function approximation seem very appealing and exciting, the sheer amount of time needed to understand and produce a functioning prototype are beyond the amount of time we have for this project. So instead, to fix this problem, we create a more simple function approximator that computes the target in terms of distances to items. From our analysis, we can see that the agent, with it’s current possible action space, seems to take unnecessary actions that yield no additional information. For example, given an episode in the initial rounds, the agent will take a path to previously visited location with no treasures or pockets of air and just randomly walk and get stuck in that particular location for an extended period of time instead of going to exploring for more treasure. This would seem to indicate that the amount of data currently being gathered is too much for the agent to handle, which is where our function approximation comes in. Rather than takes unnecessary actions our function approximator will target distances to items to alleviate the problem our agent has with taking unnecessary explorations of previously visited states. This naturally brings into question how the function will affect the amount of exploration the agent will do now. We hypothesis that there will a slight decrease in amount of exploration that will be done but not so much as to affect negatively the cumulative reward. To verify our expectations, we will provide a comparison of the different implementations we’ve made with a variety of different graphs. In regards to our current experiences, we anticipate the problems that will arise by the time the final report is due are that thinking of an idea and implementing it encompass many more problems that one might expect. While idea are simple to state, there are many problem that arise when the combination of so many different technologies are involved. We’ve realized that now after finally becoming somewhat familiar with the Malmo API. While we don’t know for certain how crippling this problem might be, we believe we’ve taken the necessary precautions to maximize our chances of completing our goal, namely, not implementing a already complex method(Neural Network), to solve a relatively simple problem. We do expect problems to arise with the more relatively simple function approximator but expect that by reducing the complexity of the proposed method that the problems that arise should be simple to solve as well. In conclusion, with our current progression of the project we don’t expect the remaining problems will cause a roadblock to our remaining goals like we previously had with our initial assumptions and expect to finish the project in a timely manner. 

### Algorithm ###
#### Q-learning algorithm ####
$$ Q(s_t,a_t)  \leftarrow \underbrace{Q(s_t,a_t)}_{\text{old value}} + \underbrace{\alpha_t}_{\text{learning rate}} \cdot \Bigg(\overbrace{\underbrace{r_{t+1}}_{\text{reward}} + \underbrace{\gamma}_{\text{discount factor}} \cdot \underbrace{\max\limits_{a} Q(s_{t+1},a)}_{\text{estimate of optimal future value}}}^{\text{learned value}}   - \underbrace{Q(s_t,a_t)}_{\text{old value}}  \Bigg)$$
#### Psuedocode ####
<img src="images/single_q_learning.png" width="600px">

#### Double Q-learning algorithm ####
$$ Q_{1}(s_t,a_t)  \leftarrow \underbrace{Q_{1}(s_t,a_t)}_{\text{old value}} + \underbrace{\alpha_t}_{\text{learning rate}} \cdot \Bigg(\overbrace{\underbrace{r_{t+1}}_{\text{reward}} +  \underbrace{\gamma}_{\text{discount factor}} \cdot \underbrace{Q_{2} \big(s_{t+1},\max\limits_{a} Q_{1}(s_{t+1},a)\big)}}^{\text{learned value}}_{\text{estimate of optimal future value}}  - \underbrace{Q_{1}(s_t,a_t)}_{\text{old value}}  \Bigg)$$
$$ Q_{2}(s_t,a_t)  \leftarrow \underbrace{Q_{2}(s_t,a_t)}_{\text{old value}} + \underbrace{\alpha_t}_{\text{learning rate}} \cdot \Bigg(\overbrace{\underbrace{r_{t+1}}_{\text{reward}} +  \underbrace{\gamma}_{\text{discount factor}} \cdot \underbrace{Q_{1} \big(s_{t+1},\max\limits_{a} Q_{2}(s_{t+1},a)\big)}}^{\text{learned value}}_{\text{estimate of optimal future value}} - \underbrace{Q_{2}(s_t,a_t)}_{\text{old value}}  \Bigg)$$
#### Psuedocode ####
<img src="images/double_q_learning.png" width="600px">

<img src="images/mdp.png" width="300px">

<img src="images/alpha.png" width="600px">
<img src="images/asymptotic.png" width="600px">
<img src="images/singleq.png" width="600px">
<img src="images/singlevsdouble.png" width="600px">
